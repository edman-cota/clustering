{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Análisis de componentes Principales\n",
    "\n",
    "### 3.1. Estudie si es posible hacer transformaciones en las variables categóricas para incluirlas en el PCA, ¿valdrá la pena?\n",
    "\n",
    "Transformaciones comunes para variables categóricas:\n",
    "1.Codificación One-Hot (dummies):\n",
    "   - Convierte cada categoría de una variable en una columna separada con valores binarios (0 o 1).\n",
    "   - Ejemplo: Si tienes una variable \"Color\" con categorías \"Rojo\", \"Verde\" y \"Azul\", se crearán tres columnas: Color_Rojo, Color_Verde, Color_Azul.\n",
    "   - Esto puede aumentar significativamente el número de características, lo cual podría dificultar la interpretación si el número de categorías es muy grande.\n",
    "\n",
    "2.Codificación de etiquetas (Label Encoding):\n",
    "   - Asigna un número único a cada categoría.\n",
    "   - Ejemplo: Si \"Rojo\" = 1, \"Verde\" = 2, \"Azul\" = 3, la variable se convierte en una variable numérica.\n",
    "   - Aunque esta es una opción más compacta, puede implicar que el modelo pueda interpretar las categorías como ordinales, \n",
    "     lo cual no es apropiado si no hay un orden natural entre las categorías.\n",
    "3.Codificación Binaria:\n",
    "   - Convierte las categorías a su representación binaria, lo cual puede ser útil si el número de categorías es grande y se desea mantener la información en un espacio más compacto.\n",
    "\n",
    "¿Vale la pena hacer estas transformaciones?\n",
    "   - Si las variables categóricas tienen un impacto significativo en la variabilidad de los datos, transformarlas puede proporcionar una representación más rica y útil para el PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Cargar los datos (suponiendo que ya tienes un DataFrame llamado 'df')\n",
    "df = pd.read_csv('movies.csv')\n",
    "\n",
    "# Verifica las columnas categóricas\n",
    "print(\"Columnas categóricas:\", df.select_dtypes(include=['object']).columns)\n",
    "\n",
    "# Codificación One-Hot\n",
    "df_encoded = pd.get_dummies(df, drop_first=True)  # drop_first=True evita multicolinealidad\n",
    "\n",
    "# Ver los primeros registros después de la codificación\n",
    "print(df_encoded.head())\n",
    "\n",
    "# Estandarización de las variables numéricas\n",
    "scaler = StandardScaler()\n",
    "df_scaled = scaler.fit_transform(df_encoded)\n",
    "\n",
    "# Mostrar la forma de los datos transformados\n",
    "print(\"Forma de los datos escalados:\", df_scaled.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Estudie si es conveniente hacer un Análisis de Componentes Principales. Recuerde que puede usar el índice KMO y el test de esfericidad de Bartlett.\n",
    "\n",
    "El PCA es útil cuando se busca reducir la dimensionalidad sin perder una cantidad significativa de información. Para evaluar si es conveniente realizar PCA, se deben utilizar dos pruebas: el índice KMO (Kaiser-Meyer-Olkin) y el test de esfericidad de Bartlett.\n",
    "\n",
    "\n",
    "1.Prueba de Esfericidad de Bartlett:\n",
    "   - Se utiliza para verificar si las variables están correlacionadas lo suficiente como para realizar un análisis factorial (PCA).\n",
    "   - Hipótesis nula: La matriz de correlación es una matriz identidad (lo que indica que no hay correlación significativa).\n",
    "   - Si el valor p de la prueba es bajo (generalmente < 0.05), se puede proceder con PCA porque las variables están correlacionadas de manera significativa.\n",
    "\n",
    "2.Índice KMO (Kaiser-Meyer-Olkin):\n",
    "   - Mide la adecuación de la muestra para el análisis factorial.\n",
    "   - El valor KMO varía entre 0 y 1, siendo:\n",
    "        - Valor KMO cercano a 1 indica que los datos son adecuados para PCA.\n",
    "        - Valor KMO menor que 0.5 indica que no es recomendable realizar PCA.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2\n",
    "import numpy as np\n",
    "\n",
    "def bartlett_test(df):\n",
    "    # Calcular la matriz de correlación\n",
    "    correlation_matrix = df.corr()\n",
    "    n = len(df)  # Número de observaciones\n",
    "    p = df.shape[1]  # Número de variables\n",
    "    \n",
    "    # Calcular el estadístico de Bartlett\n",
    "    bartlett_stat = (n - 1 - (2 * p + 5) / 6) * np.log(np.linalg.det(correlation_matrix))\n",
    "    \n",
    "    # Grados de libertad\n",
    "    df_bartlett = (p * (p - 1)) / 2\n",
    "    p_value = 1 - chi2.cdf(bartlett_stat, df_bartlett)\n",
    "    \n",
    "    return bartlett_stat, p_value\n",
    "\n",
    "# Ejecutar la prueba de Bartlett\n",
    "bartlett_stat, p_value = bartlett_test(df_encoded)\n",
    "print(f'Estadístico de Bartlett: {bartlett_stat}')\n",
    "print(f'Valor p de Bartlett: {p_value}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from factor_analyzer import calculate_kmo\n",
    "\n",
    "# Calcular el índice KMO\n",
    "kmo_all, kmo_model = calculate_kmo(df_encoded)\n",
    "print(f'Índice KMO: {kmo_model}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Haga un análisis de componentes principales con las variables numéricas, discuta los resultados e interprete los componentes.\n",
    "\n",
    "Una vez que se determina que es conveniente realizar PCA (basado en las pruebas anteriores), se procede con el análisis de los componentes principales utilizando solo las variables numéricas.\n",
    "\n",
    "\n",
    "1.Pasos para realizar PCA:\n",
    "   - Normalización de los datos: Si las variables numéricas tienen escalas diferentes, es necesario normalizarlas (por ejemplo, utilizando estandarización, para que tengan media 0 y desviación estándar 1).\n",
    "   - Cálculo de la matriz de covarianza o correlación de las variables numéricas.\n",
    "   - Extracción de los componentes principales: Usamos un algoritmo de descomposición, como la descomposición en valores singulares (SVD), para obtener los componentes principales.\n",
    "   - Evaluación de la varianza explicada por cada componente principal:\n",
    "      - Los componentes principales explican la mayor parte de la variabilidad de los datos. Se seleccionan los primeros componentes con la mayor varianza acumulada.\n",
    "      - Criterio común: Seleccionar suficientes componentes para explicar entre el 70% y el 90% de la varianza acumulada.\n",
    "\n",
    "2.Discusión de los Resultados:\n",
    "   - Componentes principales: Los componentes principales que expliquen la mayor parte de la variabilidad de los datos serán los más relevantes.\n",
    "   - Interpretación de los componentes:\n",
    "      - Carga de las variables: Cada componente tendrá una serie de coeficientes (cargas) que indican la contribución de cada variable en ese componente.\n",
    "      - Componente dominante: El primer componente generalmente será el más importante, y será una combinación lineal de las variables que más contribuyen a la variabilidad.\n",
    "   - Ejemplo de interpretación:\n",
    "      - Si el primer componente principal tiene una alta carga para las variables A, B y C, esto sugiere que estas variables están correlacionadas y explican gran parte de la variabilidad en los datos.\n",
    "      - El segundo componente puede capturar una variabilidad ortogonal a la del primero, y tendrá una carga alta en otras variables que no se correlacionan con las del primer componente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Realizar PCA\n",
    "pca = PCA()\n",
    "pca.fit(df_scaled)\n",
    "\n",
    "# Varianza explicada por cada componente\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "\n",
    "# Mostrar la varianza explicada por cada componente\n",
    "print(f'Varianza explicada por cada componente: {explained_variance}')\n",
    "\n",
    "# Gráfico de varianza explicada\n",
    "plt.plot(range(1, len(explained_variance) + 1), explained_variance, marker='o')\n",
    "plt.title('Varianza explicada por cada componente principal')\n",
    "plt.xlabel('Número de componente')\n",
    "plt.ylabel('Varianza explicada')\n",
    "plt.show()\n",
    "\n",
    "# Selección de componentes: seleccionamos los que explican al menos 80% de la varianza\n",
    "cumulative_variance = np.cumsum(explained_variance)\n",
    "num_components = np.argmax(cumulative_variance >= 0.80) + 1  # Selección del número de componentes\n",
    "print(f'Número de componentes seleccionados: {num_components}')\n",
    "\n",
    "# Obtener los componentes principales\n",
    "principal_components = pca.components_\n",
    "\n",
    "# Mostrar los coeficientes principales\n",
    "print('Coeficientes principales (cargas):')\n",
    "print(principal_components)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
